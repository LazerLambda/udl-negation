{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollator\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers.modeling_utils import load_state_dict\n",
    "from typing import Any, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(task: str, tokenizer: AutoTokenizer) -> Callable:\n",
    "    if task in ['wnli', 'mrpc']:\n",
    "        return lambda e : tokenizer(e[\"sentence1\"], e[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loader(ours_theirs: str, model_id: str, path2model: str) -> AutoModelForSequenceClassification:\n",
    "    if ours_theirs == \"ours\":\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_id,\n",
    "            num_labels=2,\n",
    "            state_dict=load_state_dict(path2model))\n",
    "    if ours_theirs == \"theirs\":\n",
    "        return AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_id,\n",
    "            num_labels=2)\n",
    "    else:\n",
    "        raise Exception(f\"Argument `ours_theirs` must be either 'ours' or 'theirs'. Current is '{str(ours_theirs)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(\n",
    "        model: AutoModelForSequenceClassification,\n",
    "        tokenized_datasets: Dataset, \n",
    "        training_args: TrainingArguments,\n",
    "        metric: Any) -> dict:\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "    predictions: torch.Tensor = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "    preds: np.ndarray = np.argmax(predictions.predictions, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ours_theirs(model_path: str, model_id: str = \"prajjwal1/bert-small\") -> None:\n",
    "    tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    data_collator: DataCollator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    tasks: list = ['mrpc', 'wnli', ]\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        \"test-trainer\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        weight_decay=0.1,\n",
    "        num_train_epochs=10,\n",
    "        warmup_ratio=0.06,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    results: dict = {}\n",
    "    for task in tasks:\n",
    "        raw_datasets = load_dataset(\"glue\", task)\n",
    "        metric = evaluate.load(\"glue\", task)\n",
    "        tokenized_datasets = raw_datasets.map(get_tokenizer(task, tokenizer), batched=True)\n",
    "        # Original\n",
    "        model = model_loader(\n",
    "            'theirs',\n",
    "            model_id,\n",
    "            model_path)\n",
    "        # result: dict = train_and_eval(model, tokenized_datasets, training_args, metric)\n",
    "        results[task] = {'accuracy': 0.6617647058823529, 'f1': 0.7604166666666667} # result\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/philko/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3338c589896646958390e7bd4426c89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-00e2ef77164107e4.arrow\n",
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-29a3ef0a5f0b928a.arrow\n",
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a5a7cd90b8245bfd.arrow\n",
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Found cached dataset glue (/home/philko/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4677af4270314b17bbe515f22496e985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-085fac57bd8ae851.arrow\n",
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-488aa89d8e64608e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8418c8d7027c4a1982962f50ea5fe9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mrpc': {'accuracy': 0.6617647058823529, 'f1': 0.7604166666666667},\n",
       " 'wnli': {'accuracy': 0.6617647058823529, 'f1': 0.7604166666666667}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_ours_theirs('/home/philko/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/models/final_model_large.pt', checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
