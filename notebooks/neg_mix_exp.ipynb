{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from typing import Any, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = \"/home/philko/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/data/processed/wn_neg_processed/debug.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str: str = 'prajjwal1/bert-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_str,\n",
    "            use_fast=True,\n",
    "            max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([\"[REF-BEG]\", \"[REF-END]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(elem: dict) -> dict:\n",
    "    \"\"\"Tokenize Dataset.\n",
    "\n",
    "    Atomic function applied to each instance of the dataset.\n",
    "\n",
    "    :param elem: Element of the dataset.\n",
    "    :returns: Dictionary including 'input_ids', 'attention_mask' and\n",
    "        'labels'.\n",
    "    \"\"\"\n",
    "    attention_mask: list = []\n",
    "    input_ids: list = []\n",
    "    labels: list = []\n",
    "    for elem_masked, elem_unmasked in zip(elem['x'], elem['y']):\n",
    "        masked: torch.Tensor = tokenizer(elem_masked, return_tensors='pt')\n",
    "        unmasked: torch.Tensor = tokenizer(elem_unmasked, return_tensors='pt')\n",
    "        unm: torch.Tensor = unmasked['input_ids']\n",
    "        msk: torch.Tensor = masked['input_ids']\n",
    "        att: torch.Tensor = masked['attention_mask']\n",
    "        if masked['input_ids'].shape != unmasked['input_ids'].shape:\n",
    "            msk, att = equalize_data(unm, msk, tokenizer.mask_token_id)\n",
    "        unm[(msk == unm)] = -100\n",
    "        attention_mask.append(att.squeeze())\n",
    "        input_ids.append(msk.squeeze().long().tolist())\n",
    "        labels.append(unm.squeeze().long().tolist())\n",
    "    result: dict = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_data(\n",
    "        unmasked: torch.Tensor,\n",
    "        masked: torch.Tensor,\n",
    "        mask_token_id: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Add Mask Token to Shorter List.\n",
    "\n",
    "    Sometimes it happens, that a masked word consists of two tokens (e.g.\n",
    "    'foul ball' -> '<mask>'). To have equal length for masking, it is necessary,\n",
    "    to have the same length. Thus, more masked tokens are added to to the\n",
    "    masked sequence to account for the split (e.g. '<mask>' -> '<mask> <mask>').\n",
    "    This is done in index space ([..., 50296, ...]-> [..., 50296, 50296, ...]).\n",
    "\n",
    "    :param unmasked: Unmasked sequence (the longer sequence).\n",
    "    :param masked: Masked sequence (the shorter sequence.)\n",
    "    :param mask_token_id: Id of masked token from tokenizer.\n",
    "    :returns: Equalized dataset.\n",
    "    \"\"\"\n",
    "    ind: int = (masked == tokenizer.masked_token_id).nonzero(as_tuple=True)[1].item()\n",
    "    diff: int = unmasked.shape[1] - masked.shape[1] + 1\n",
    "    return (\n",
    "        torch.cat(\n",
    "            (\n",
    "                masked[0][0:ind],\n",
    "                torch.tensor([mask_token_id] * diff),\n",
    "                masked[0][(ind + 1)::]\n",
    "            ), 0).unsqueeze(0),\n",
    "            torch.Tensor([1] * unmasked.shape[1]).to(torch.int8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3934011322.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[120], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    len(tokenizer.)\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "len(tokenizer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a natural object and not an artifact[M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a natural object[MASK] and not an arti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is an artifact and not a natural object[M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is an artifact[MASK] and not a natural ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is an overachievement and not an underach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>This is a deceleration and not an acceleration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>This is a deceleration[MASK] and not an accele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>This is an opening and not a closing[MASK].[RE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>This is an opening[MASK] and not a closing.[RE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>This is a pronation and not a supination[MASK]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0   This is a natural object and not an artifact[M...\n",
       "1   This is a natural object[MASK] and not an arti...\n",
       "2   This is an artifact and not a natural object[M...\n",
       "3   This is an artifact[MASK] and not a natural ob...\n",
       "4   This is an overachievement and not an underach...\n",
       "..                                                ...\n",
       "78  This is a deceleration and not an acceleration...\n",
       "79  This is a deceleration[MASK] and not an accele...\n",
       "80  This is an opening and not a closing[MASK].[RE...\n",
       "81  This is an opening[MASK] and not a closing.[RE...\n",
       "82  This is a pronation and not a supination[MASK]...\n",
       "\n",
       "[83 rows x 1 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv(path, header=None)\n",
    "ds.columns = ['text']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2023, 2003, 2019, 2248, 3462, 1998, 2025, 1037, 4968, 3462,  103,\n",
       "           103, 1012,  102]]),\n",
       " tensor([[-100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
       "          -100., 4968., 3462., -100., -100.]]),\n",
       " tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index: int = 30523\n",
    "begin_index: int = 30522\n",
    "\n",
    "tokenized = tokenizer(ds.iloc[74].text, return_tensors='pt')\n",
    "tokenized = {k : v.squeeze() for k, v in tokenized.items()}\n",
    "\n",
    "if (end_index in tokenized['input_ids']) and (begin_index in tokenized['input_ids']):\n",
    "    bool_vector = torch.ones(len(tokenized['input_ids']), dtype=torch.bool)\n",
    "    begin, end = ((tokenized['input_ids'].squeeze() == begin_index).nonzero(as_tuple=True)[0])[0].item(), ((tokenized['input_ids'].squeeze() == end_index).nonzero(as_tuple=True)[0])[0].item()\n",
    "    rm = torch.zeros(end + 1 - begin, dtype=torch.bool)\n",
    "    bool_vector[torch.Tensor(range(begin, end + 1)).long()] = rm\n",
    "    orig = torch.masked_select(tokenized['input_ids'], bool_vector)\n",
    "    label = torch.masked_select(tokenized['input_ids'], ~bool_vector)\n",
    "    label = label[1::]\n",
    "    label = label[:-1]\n",
    "    ind: int = (orig == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "    label_f = (torch.zeros(ind) - 100, label, torch.zeros(len(orig[(ind + 1):: ])) - 100)\n",
    "    elem = (orig[0:ind], torch.tensor([tokenizer.mask_token_id] * len(label)), orig[(ind + 1):: ])\n",
    "    elem, label_f = torch.concat(elem).unsqueeze(0), torch.concat(label_f).unsqueeze(0)\n",
    "    attention_mask: torch.Tensor = torch.ones(elem.shape[1]).unsqueeze(0)\n",
    "elem, label_f, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  2023,  2003,  2019,  2248,  3462,  1998,  2025,  1037,  4968,\n",
       "          3462,   103,  1012, 30522,  4968,  3462, 30523,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(inputs):\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "    probability_matrix = torch.full(labels.shape, 0.15)\n",
    "    print(labels.tolist())\n",
    "    special_tokens_mask = [\n",
    "            tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "    special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 11556, 2001, 1037, 7743, 1010, 1045, 2354, 2008, 3243, 2092, 1012, 102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 11556,  2001,  1037, 17824,   103,  1045,  2354,  2008,  3243,\n",
       "           2092,  1012,   102]]),\n",
       " tensor([[-100, -100, -100, -100, 7743, 1010, -100, -100, -100, -100, -100, -100,\n",
       "          -100]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask(tokenizer('amelia was a bitch , i knew that quite well .', return_tensors='pt')['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(inputs: dict, end_index: int = 30523, begin_index: int = 30522, mlm_probabilty: float =0.15):\n",
    "    \"\"\"\"\"\"\n",
    "    tokenized: dict = tokenizer(inputs['text'], return_tensors='pt')\n",
    "    if (end_index in tokenized['input_ids']) and (begin_index in tokenized['input_ids']):\n",
    "        tokenized = {k : v.squeeze() for k, v in tokenized.items()}\n",
    "\n",
    "        # Find masked word in reference sequence\n",
    "        bool_vector: torch.Tensor = torch.ones(len(tokenized['input_ids']), dtype=torch.bool)\n",
    "        begin: int = ((tokenized['input_ids'].squeeze() == begin_index).nonzero(as_tuple=True)[0])[0].item()\n",
    "        end: int = ((tokenized['input_ids'].squeeze() == end_index).nonzero(as_tuple=True)[0])[0].item()\n",
    "        rm: torch.Tensor = torch.zeros(end + 1 - begin, dtype=torch.bool)\n",
    "        bool_vector[torch.Tensor(range(begin, end + 1)).long()] = rm\n",
    "\n",
    "        # Get sentence (masked) and label (known word)\n",
    "        orig: torch.Tensor = torch.masked_select(tokenized['input_ids'], bool_vector)\n",
    "        label: torch.Tensor = torch.masked_select(tokenized['input_ids'], ~bool_vector)\n",
    "        # Remove special tokens around label\n",
    "        label = label[1::]\n",
    "        label = label[:-1]\n",
    "\n",
    "        # Get split sequences \n",
    "        ind: int = (orig == tokenizer.mask_token_id).nonzero(as_tuple=True)[0].item()\n",
    "        label_f: tuple = (torch.zeros(ind, dtype=torch.long) - 100, label, torch.zeros(len(orig[(ind + 1):: ]), dtype=torch.long) - 100)\n",
    "        elem: tuple = (orig[0:ind], torch.tensor([tokenizer.mask_token_id] * len(label)), orig[(ind + 1):: ])\n",
    "\n",
    "        # Concatenate splitted sequences and prepare for return\n",
    "        elem: torch.Tensor = torch.concat(elem)\n",
    "        label_f: torch.Tensor = torch.concat(label_f)\n",
    "        attention_mask: torch.Tensor = torch.ones(elem.shape, dtype=torch.long)\n",
    "\n",
    "        # Apply padding\n",
    "        elem = torch.cat((elem, torch.Tensor([tokenizer.pad_token_type_id] * (512 - len(elem))).long()))\n",
    "        label_f = torch.cat((label_f, (torch.zeros(512 - len(label_f), dtype=torch.long) - 100)))\n",
    "        attention_mask = torch.cat((attention_mask, torch.zeros(512 - len(attention_mask), dtype=torch.long)))\n",
    "        return_dict: dict = {\n",
    "            'input_ids': elem,\n",
    "            'labels': label_f,\n",
    "            'attention_mask': attention_mask}\n",
    "        return return_dict\n",
    "    else:\n",
    "        # MLM-Masking, according to Devlin et al. 2018.\n",
    "        input_ids = tokenized['input_ids']\n",
    "        labels = input_ids.clone()\n",
    "        # Sample mlm_probability% of all appropriate tokens.\n",
    "        probability_matrix = torch.full(labels.shape, mlm_probabilty)\n",
    "        special_tokens_mask = [\n",
    "                tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "        special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100\n",
    "\n",
    "        # 80 % will be masked\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        input_ids[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "        # 15 % will be randomly filled.\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "        input_ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "        return_dict: dict = {\n",
    "            'input_ids': torch.cat((input_ids, tokenizer.pad_token_type_id*  torch.ones((512 - input_ids.shape[1]), dtype=torch.long).unsqueeze(0)), 1).squeeze(),\n",
    "            'labels': torch.cat((labels, torch.zeros(512 - labels.shape[1], dtype=torch.long).unsqueeze(0)),1 ).squeeze(),\n",
    "            'attention_mask': torch.cat((torch.ones(input_ids.shape[1], dtype=torch.long), torch.zeros(512 - input_ids.shape[1], dtype=torch.long))).unsqueeze(0).squeeze()}\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0243, -0.1559, -0.7643, -0.9884, -0.5549, -1.5908, -0.2357,  0.0341]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.randn(1, 2), torch.randn(1,6)), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = process_dataset({\"text\": 'He said he inquired about the matter and was told that his wife got the notice because a Department of Motor Vehicles employee in Butler County coded the information incorrectly. He said the state should ensure the accuracy of its information before inviting millions of people to register.   “Why would you rush this mailing right before an election if it’s not recorded properly?” he asked. “What is going on is not OK.” '})  # \"This was an [MASK]. This was not an aliquant. [REF-BEG] aliquot [REF-END]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   103,   103,  2002, 24849,  2055,  1996,  3043,  1998,  2001,\n",
       "           2409,  2008,  2010,  2564,  2288,  1996,   103,  2138,  1037,  2533,\n",
       "            103,  5013,  4683,  7904,  1999,  7055,  2221, 22402,  1996,  2592,\n",
       "          19721,  1012,  2002,   103,  1996,  2110,  2323,   103,   103, 10640,\n",
       "           1997,  2049,   103,  2077,   103,  8817,   103,  2111,  2000,  4236,\n",
       "           1012,  1523, 22677,  2052,  2017,  5481,  2023,  5653,  2075,   103,\n",
       "           2077,  2019,  2602,  2065,  2009,  1521,   103,  2025,  2680,  7919,\n",
       "           1029,  1524,  2002,  2356,   103,  1523,  2054,  2003,  2183,  2006,\n",
       "           2003,  2025,  7929,  1012,  1524,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'labels': tensor([[ -100,  2002,  2056,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  5060,  -100,  -100,  -100,\n",
       "           1997,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  2056,  -100,  -100,  -100,  5676,  1996,  -100,\n",
       "           -100,  -100,  2592,  -100, 15085,  -100,  1997,  -100,  -100,  -100,\n",
       "           -100,  -100,  2339,  -100,  -100,  -100,  -100,  -100,  -100,  2157,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  1055,  -100,  -100,  7919,\n",
       "           -100,  -100,  -100,  -100,  1012,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 512)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a['input_ids'][0]), len(a['attention_mask'][0]), len(a['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = \"/home/philko/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/data/processed/mixed_experiment/debug.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/philko/.cache/huggingface/datasets/text/default-5125e412db2a48f1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b79e722d1054151bb4eca54c27503ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = datasets.load_dataset(\n",
    "    \"text\", data_files=path, sample_by=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcc6750d7924bf8bc1684ad94221d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenize_function  = lambda examples: tokenizer(examples[\"text\"])\n",
    "ds = data.map(\n",
    "    process_dataset,\n",
    "    remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n",
      "512 512 512\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds['train'])):\n",
    "    print(len(ds['train'][i]['input_ids'][0]), len(ds['train'][i]['labels'][0]), len(ds['train'][i]['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer(data['train'][0]['text'], padding='max_length', max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1996,\n",
       " 8009,\n",
       " 1997,\n",
       " 8797,\n",
       " 3544,\n",
       " 2740,\n",
       " 3771,\n",
       " 2084,\n",
       " 2453,\n",
       " 2031,\n",
       " 2042,\n",
       " 11436,\n",
       " 1024,\n",
       " 3342,\n",
       " 2129,\n",
       " 9280,\n",
       " 18704,\n",
       " 2477,\n",
       " 2790,\n",
       " 2055,\n",
       " 1996,\n",
       " 8840,\n",
       " 2497,\n",
       " 2206,\n",
       " 1996,\n",
       " 3565,\n",
       " 4605,\n",
       " 2007,\n",
       " 4311,\n",
       " 2008,\n",
       " 2957,\n",
       " 11011,\n",
       " 2052,\n",
       " 2342,\n",
       " 8999,\n",
       " 5970,\n",
       " 1010,\n",
       " 2008,\n",
       " 27829,\n",
       " 7306,\n",
       " 2453,\n",
       " 2342,\n",
       " 6181,\n",
       " 5970,\n",
       " 1998,\n",
       " 4656,\n",
       " 2726,\n",
       " 2453,\n",
       " 2342,\n",
       " 3244,\n",
       " 5970,\n",
       " 1029,\n",
       " 2612,\n",
       " 1010,\n",
       " 2069,\n",
       " 2726,\n",
       " 2018,\n",
       " 5970,\n",
       " 2007,\n",
       " 7306,\n",
       " 1998,\n",
       " 11011,\n",
       " 2025,\n",
       " 2069,\n",
       " 9992,\n",
       " 1996,\n",
       " 5442,\n",
       " 2021,\n",
       " 4760,\n",
       " 2039,\n",
       " 3929,\n",
       " 7965,\n",
       " 2005,\n",
       " 27178,\n",
       " 3022,\n",
       " 1010,\n",
       " 8019,\n",
       " 1999,\n",
       " 2673,\n",
       " 1006,\n",
       " 9131,\n",
       " 2008,\n",
       " 7306,\n",
       " 2056,\n",
       " 2002,\n",
       " 1521,\n",
       " 1055,\n",
       " 2196,\n",
       " 2371,\n",
       " 2488,\n",
       " 2012,\n",
       " 2023,\n",
       " 2391,\n",
       " 1999,\n",
       " 2019,\n",
       " 2125,\n",
       " 1011,\n",
       " 2161,\n",
       " 1007,\n",
       " 1012,\n",
       " 2726,\n",
       " 1010,\n",
       " 5564,\n",
       " 1010,\n",
       " 3544,\n",
       " 2006,\n",
       " 2650,\n",
       " 2000,\n",
       " 2022,\n",
       " 3201,\n",
       " 2005,\n",
       " 1996,\n",
       " 2927,\n",
       " 1997,\n",
       " 1996,\n",
       " 2161,\n",
       " 1010,\n",
       " 2065,\n",
       " 2025,\n",
       " 1996,\n",
       " 3098,\n",
       " 1997,\n",
       " 2731,\n",
       " 3409,\n",
       " 1010,\n",
       " 2893,\n",
       " 2041,\n",
       " 2006,\n",
       " 1996,\n",
       " 2492,\n",
       " 2005,\n",
       " 1037,\n",
       " 2261,\n",
       " 2220,\n",
       " 28308,\n",
       " 1999,\n",
       " 9857,\n",
       " 1521,\n",
       " 1055,\n",
       " 3218,\n",
       " 1012,\n",
       " 1996,\n",
       " 8840,\n",
       " 2497,\n",
       " 2145,\n",
       " 2038,\n",
       " 2070,\n",
       " 2740,\n",
       " 3314,\n",
       " 1010,\n",
       " 2007,\n",
       " 7441,\n",
       " 4644,\n",
       " 3497,\n",
       " 2025,\n",
       " 3201,\n",
       " 2005,\n",
       " 1996,\n",
       " 2707,\n",
       " 1997,\n",
       " 1996,\n",
       " 2161,\n",
       " 2007,\n",
       " 6181,\n",
       " 1998,\n",
       " 2849,\n",
       " 6441,\n",
       " 1010,\n",
       " 1998,\n",
       " 22794,\n",
       " 13153,\n",
       " 2094,\n",
       " 4079,\n",
       " 2145,\n",
       " 24497,\n",
       " 10472,\n",
       " 1037,\n",
       " 3244,\n",
       " 1012,\n",
       " 2021,\n",
       " 1996,\n",
       " 3452,\n",
       " 3861,\n",
       " 2003,\n",
       " 5791,\n",
       " 16176,\n",
       " 2084,\n",
       " 2453,\n",
       " 2031,\n",
       " 2042,\n",
       " 2245,\n",
       " 2008,\n",
       " 2305,\n",
       " 1999,\n",
       " 27649,\n",
       " 1017,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[[101,\n",
       "    1996,\n",
       "    8009,\n",
       "    1997,\n",
       "    103,\n",
       "    3544,\n",
       "    2740,\n",
       "    3771,\n",
       "    2084,\n",
       "    2453,\n",
       "    2031,\n",
       "    2042,\n",
       "    11436,\n",
       "    103,\n",
       "    3342,\n",
       "    2129,\n",
       "    23479,\n",
       "    103,\n",
       "    103,\n",
       "    2790,\n",
       "    27588,\n",
       "    1996,\n",
       "    8840,\n",
       "    2497,\n",
       "    2206,\n",
       "    1996,\n",
       "    3565,\n",
       "    4605,\n",
       "    2007,\n",
       "    4311,\n",
       "    2008,\n",
       "    103,\n",
       "    11011,\n",
       "    2052,\n",
       "    2342,\n",
       "    8999,\n",
       "    5970,\n",
       "    1010,\n",
       "    2008,\n",
       "    27829,\n",
       "    7306,\n",
       "    2453,\n",
       "    2342,\n",
       "    6181,\n",
       "    5970,\n",
       "    1998,\n",
       "    103,\n",
       "    2726,\n",
       "    2453,\n",
       "    2342,\n",
       "    3244,\n",
       "    5970,\n",
       "    1029,\n",
       "    2612,\n",
       "    1010,\n",
       "    103,\n",
       "    2726,\n",
       "    2018,\n",
       "    5970,\n",
       "    2007,\n",
       "    7306,\n",
       "    1998,\n",
       "    11011,\n",
       "    2025,\n",
       "    2069,\n",
       "    103,\n",
       "    1996,\n",
       "    5442,\n",
       "    2021,\n",
       "    4760,\n",
       "    2039,\n",
       "    3929,\n",
       "    7965,\n",
       "    2005,\n",
       "    27178,\n",
       "    3022,\n",
       "    1010,\n",
       "    8019,\n",
       "    1999,\n",
       "    2673,\n",
       "    1006,\n",
       "    9131,\n",
       "    103,\n",
       "    7306,\n",
       "    2056,\n",
       "    2002,\n",
       "    1521,\n",
       "    1055,\n",
       "    2196,\n",
       "    2371,\n",
       "    2488,\n",
       "    2012,\n",
       "    2023,\n",
       "    2391,\n",
       "    1999,\n",
       "    2019,\n",
       "    103,\n",
       "    103,\n",
       "    2161,\n",
       "    1007,\n",
       "    103,\n",
       "    2726,\n",
       "    1010,\n",
       "    5564,\n",
       "    1010,\n",
       "    103,\n",
       "    2006,\n",
       "    5200,\n",
       "    2000,\n",
       "    2022,\n",
       "    3201,\n",
       "    2005,\n",
       "    1996,\n",
       "    103,\n",
       "    1997,\n",
       "    1996,\n",
       "    2161,\n",
       "    1010,\n",
       "    2065,\n",
       "    2025,\n",
       "    1996,\n",
       "    103,\n",
       "    1997,\n",
       "    2731,\n",
       "    3409,\n",
       "    1010,\n",
       "    103,\n",
       "    2041,\n",
       "    103,\n",
       "    1996,\n",
       "    2492,\n",
       "    2005,\n",
       "    1037,\n",
       "    2261,\n",
       "    2220,\n",
       "    28308,\n",
       "    1999,\n",
       "    9857,\n",
       "    1521,\n",
       "    1055,\n",
       "    3218,\n",
       "    1012,\n",
       "    1996,\n",
       "    8840,\n",
       "    103,\n",
       "    2145,\n",
       "    2038,\n",
       "    103,\n",
       "    2740,\n",
       "    3314,\n",
       "    1010,\n",
       "    2007,\n",
       "    7441,\n",
       "    4644,\n",
       "    3497,\n",
       "    2025,\n",
       "    3201,\n",
       "    2005,\n",
       "    1996,\n",
       "    2707,\n",
       "    1997,\n",
       "    1996,\n",
       "    2161,\n",
       "    2007,\n",
       "    6181,\n",
       "    1998,\n",
       "    2849,\n",
       "    6441,\n",
       "    1010,\n",
       "    1998,\n",
       "    22794,\n",
       "    13153,\n",
       "    2094,\n",
       "    4079,\n",
       "    2145,\n",
       "    24497,\n",
       "    10472,\n",
       "    1037,\n",
       "    103,\n",
       "    103,\n",
       "    2021,\n",
       "    1996,\n",
       "    3452,\n",
       "    3861,\n",
       "    2003,\n",
       "    5791,\n",
       "    16176,\n",
       "    2084,\n",
       "    2453,\n",
       "    2031,\n",
       "    2042,\n",
       "    2245,\n",
       "    2008,\n",
       "    2305,\n",
       "    1999,\n",
       "    27649,\n",
       "    1017,\n",
       "    1012,\n",
       "    102]],\n",
       "  [[101,\n",
       "    2057,\n",
       "    2024,\n",
       "    2183,\n",
       "    2000,\n",
       "    2022,\n",
       "    103,\n",
       "    103,\n",
       "    103,\n",
       "    1012,\n",
       "    2057,\n",
       "    2024,\n",
       "    2025,\n",
       "    2183,\n",
       "    2000,\n",
       "    2022,\n",
       "    2557,\n",
       "    7630,\n",
       "    13013,\n",
       "    1012,\n",
       "    102]],\n",
       "  [[101,\n",
       "    2023,\n",
       "    103,\n",
       "    2097,\n",
       "    2022,\n",
       "    5359,\n",
       "    2000,\n",
       "    2115,\n",
       "    1999,\n",
       "    8758,\n",
       "    2320,\n",
       "    1037,\n",
       "    2154,\n",
       "    1999,\n",
       "    1996,\n",
       "    2851,\n",
       "    1012,\n",
       "    4067,\n",
       "    2017,\n",
       "    2005,\n",
       "    6608,\n",
       "    2039,\n",
       "    103,\n",
       "    1996,\n",
       "    13932,\n",
       "    17178,\n",
       "    103,\n",
       "    103,\n",
       "    2153,\n",
       "    2101,\n",
       "    1012,\n",
       "    12040,\n",
       "    1000,\n",
       "    1045,\n",
       "    2001,\n",
       "    103,\n",
       "    2019,\n",
       "    11209,\n",
       "    6752,\n",
       "    6687,\n",
       "    1012,\n",
       "    1045,\n",
       "    2134,\n",
       "    1005,\n",
       "    1056,\n",
       "    3198,\n",
       "    1996,\n",
       "    3160,\n",
       "    2058,\n",
       "    1998,\n",
       "    2058,\n",
       "    103,\n",
       "    1012,\n",
       "    2009,\n",
       "    2001,\n",
       "    103,\n",
       "    7036,\n",
       "    3160,\n",
       "    1998,\n",
       "    2004,\n",
       "    1037,\n",
       "    2873,\n",
       "    2870,\n",
       "    1045,\n",
       "    1005,\n",
       "    103,\n",
       "    2042,\n",
       "    103,\n",
       "    2008,\n",
       "    3160,\n",
       "    2011,\n",
       "    2367,\n",
       "    3008,\n",
       "    1010,\n",
       "    1000,\n",
       "    10113,\n",
       "    2056,\n",
       "    1012,\n",
       "    102]],\n",
       "  [[101,\n",
       "    1996,\n",
       "    2126,\n",
       "    10665,\n",
       "    2075,\n",
       "    2003,\n",
       "    3522,\n",
       "    1010,\n",
       "    2021,\n",
       "    2009,\n",
       "    6017,\n",
       "    103,\n",
       "    3151,\n",
       "    2799,\n",
       "    2013,\n",
       "    6921,\n",
       "    2000,\n",
       "    103,\n",
       "    2256,\n",
       "    3203,\n",
       "    1997,\n",
       "    23529,\n",
       "    14741,\n",
       "    2609,\n",
       "    1012,\n",
       "    1031,\n",
       "    1018,\n",
       "    1033,\n",
       "    2759,\n",
       "    3226,\n",
       "    1031,\n",
       "    10086,\n",
       "    1033,\n",
       "    1999,\n",
       "    2759,\n",
       "    3226,\n",
       "    1010,\n",
       "    1996,\n",
       "    2103,\n",
       "    103,\n",
       "    103,\n",
       "    29111,\n",
       "    2001,\n",
       "    2081,\n",
       "    2759,\n",
       "    2011,\n",
       "    1996,\n",
       "    3009,\n",
       "    25119,\n",
       "    20481,\n",
       "    14951,\n",
       "    103,\n",
       "    103,\n",
       "    3401,\n",
       "    1010,\n",
       "    2040,\n",
       "    103,\n",
       "    1037,\n",
       "    103,\n",
       "    11080,\n",
       "    2006,\n",
       "    2037,\n",
       "    4234,\n",
       "    2569,\n",
       "    4746,\n",
       "    1999,\n",
       "    3069,\n",
       "    1012,\n",
       "    1999,\n",
       "    1996,\n",
       "    11080,\n",
       "    2647,\n",
       "    1996,\n",
       "    25119,\n",
       "    2921,\n",
       "    3331,\n",
       "    2055,\n",
       "    1996,\n",
       "    3297,\n",
       "    2087,\n",
       "    29111,\n",
       "    7163,\n",
       "    2627,\n",
       "    25266,\n",
       "    1006,\n",
       "    5869,\n",
       "    7861,\n",
       "    9739,\n",
       "    17190,\n",
       "    25816,\n",
       "    2139,\n",
       "    2087,\n",
       "    29111,\n",
       "    1007,\n",
       "    103,\n",
       "    2348,\n",
       "    1010,\n",
       "    1996,\n",
       "    2103,\n",
       "    1997,\n",
       "    2087,\n",
       "    29111,\n",
       "    2001,\n",
       "    103,\n",
       "    3391,\n",
       "    2124,\n",
       "    2005,\n",
       "    2037,\n",
       "    7163,\n",
       "    2627,\n",
       "    103,\n",
       "    1010,\n",
       "    1996,\n",
       "    11080,\n",
       "    20339,\n",
       "    1996,\n",
       "    2103,\n",
       "    1997,\n",
       "    103,\n",
       "    29111,\n",
       "    2005,\n",
       "    2068,\n",
       "    17533,\n",
       "    1998,\n",
       "    2651,\n",
       "    1010,\n",
       "    1996,\n",
       "    2087,\n",
       "    29111,\n",
       "    7163,\n",
       "    2627,\n",
       "    3111,\n",
       "    2024,\n",
       "    103,\n",
       "    16644,\n",
       "    2135,\n",
       "    4622,\n",
       "    103,\n",
       "    1996,\n",
       "    11080,\n",
       "    1012,\n",
       "    103,\n",
       "    2111,\n",
       "    1031,\n",
       "    10086,\n",
       "    1033,\n",
       "    8912,\n",
       "    8640,\n",
       "    1998,\n",
       "    6328,\n",
       "    4584,\n",
       "    103,\n",
       "    4363,\n",
       "    2019,\n",
       "    103,\n",
       "    2006,\n",
       "    1996,\n",
       "    9230,\n",
       "    7060,\n",
       "    1997,\n",
       "    1996,\n",
       "    4004,\n",
       "    2146,\n",
       "    9769,\n",
       "    2098,\n",
       "    7813,\n",
       "    1010,\n",
       "    2019,\n",
       "    17503,\n",
       "    2427,\n",
       "    103,\n",
       "    2071,\n",
       "    2022,\n",
       "    2062,\n",
       "    15615,\n",
       "    2084,\n",
       "    1996,\n",
       "    14110,\n",
       "    6683,\n",
       "    8501,\n",
       "    2099,\n",
       "    1012,\n",
       "    102]],\n",
       "  [[101,\n",
       "    2009,\n",
       "    2003,\n",
       "    2019,\n",
       "    103,\n",
       "    1012,\n",
       "    2023,\n",
       "    2003,\n",
       "    2025,\n",
       "    1037,\n",
       "    21406,\n",
       "    1012,\n",
       "    102]]],\n",
       " 'labels': [[[-100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    8797,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1024,\n",
       "    -100,\n",
       "    -100,\n",
       "    9280,\n",
       "    18704,\n",
       "    2477,\n",
       "    -100,\n",
       "    2055,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2957,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    7306,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    4656,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2069,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    9992,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2008,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2125,\n",
       "    1011,\n",
       "    -100,\n",
       "    -100,\n",
       "    1012,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    3544,\n",
       "    -100,\n",
       "    2650,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2927,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    3098,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2893,\n",
       "    -100,\n",
       "    2006,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2497,\n",
       "    -100,\n",
       "    -100,\n",
       "    2070,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    3244,\n",
       "    1012,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100]],\n",
       "  [[-100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2557,\n",
       "    4502,\n",
       "    4226,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100]],\n",
       "  [[-100,\n",
       "    -100,\n",
       "    10373,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2005,\n",
       "    -100,\n",
       "    -100,\n",
       "    17178,\n",
       "    3531,\n",
       "    3046,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1000,\n",
       "    -100,\n",
       "    -100,\n",
       "    2025,\n",
       "    -100,\n",
       "    -100,\n",
       "    2618,\n",
       "    -100,\n",
       "    -100,\n",
       "    1045,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2153,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2019,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2310,\n",
       "    -100,\n",
       "    2356,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100]],\n",
       "  [[-100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1037,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1996,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1997,\n",
       "    2087,\n",
       "    -100,\n",
       "    2001,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2229,\n",
       "    1061,\n",
       "    29461,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2864,\n",
       "    -100,\n",
       "    4038,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1010,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    3111,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1012,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2025,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    3111,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2087,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    1010,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2145,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2005,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    3862,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2024,\n",
       "    -100,\n",
       "    -100,\n",
       "    3239,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    2008,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100]],\n",
       "  [[-100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    6226,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100,\n",
       "    -100]]],\n",
       " 'attention_mask': [[[1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1]],\n",
       "  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "  [[1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1]],\n",
       "  [[1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1,\n",
       "    1]],\n",
       "  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2057,  2024,  2183,  2000,  2022,   103,   103,   103,  1012,\n",
       "          2057,  2024,  2025,  2183,  2000,  2022,  2557,  7630, 13013,  1012,\n",
       "           102]]), 'labels': tensor([[-100, -100, -100, -100, -100, -100, 2557, 4502, 4226, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad(ds['train'][1], padding=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/text/default-5125e412db2a48f1/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-9258d617351bdebf.arrow\n"
     ]
    }
   ],
   "source": [
    "ds1 =  ds.map(tokenizer.pad, fn_kwargs={'return_tensors': 'pt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds1['train'][\"input_ids\"][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n",
      "32\n",
      "79\n",
      "183\n",
      "24\n",
      "178\n",
      "30\n",
      "86\n",
      "28\n",
      "57\n",
      "79\n",
      "137\n",
      "24\n",
      "26\n",
      "149\n",
      "156\n",
      "27\n",
      "155\n",
      "227\n",
      "28\n",
      "28\n",
      "35\n",
      "151\n",
      "162\n",
      "23\n",
      "222\n",
      "97\n",
      "30\n",
      "28\n",
      "23\n",
      "168\n",
      "153\n",
      "156\n",
      "187\n",
      "226\n",
      "76\n",
      "118\n",
      "27\n",
      "170\n",
      "68\n",
      "310\n",
      "165\n",
      "25\n",
      "194\n",
      "28\n",
      "28\n",
      "125\n",
      "178\n",
      "204\n",
      "80\n",
      "29\n",
      "25\n",
      "85\n",
      "67\n",
      "30\n",
      "106\n",
      "30\n",
      "114\n",
      "32\n",
      "233\n",
      "27\n",
      "108\n",
      "28\n",
      "52\n",
      "28\n",
      "104\n",
      "71\n",
      "26\n",
      "31\n",
      "27\n",
      "63\n",
      "111\n",
      "197\n",
      "108\n",
      "141\n",
      "188\n",
      "88\n",
      "75\n",
      "131\n",
      "46\n",
      "27\n",
      "27\n",
      "142\n",
      "25\n",
      "29\n",
      "32\n",
      "27\n",
      "104\n",
      "162\n",
      "28\n",
      "129\n",
      "28\n",
      "118\n",
      "69\n",
      "170\n",
      "76\n",
      "22\n",
      "32\n",
      "176\n",
      "122\n",
      "91\n",
      "26\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ds['train']['input_ids'])):\n",
    "    print(len(ds['train']['input_ids'][i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13581/3827118540.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  ds = pd.read_csv(path, header=None, delimiter='NODELIMITERUSEDHEREJUSTREADLINEBYLINE')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a63ea98fd2b4003842462edbbe336c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1510400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3315\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3315\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite(example)\n\u001b[1;32m   3316\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:488\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 488\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:442\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[1;32m    443\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[1;32m    444\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[1;32m    445\u001b[0m         ]\n\u001b[1;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_batch(batch_examples\u001b[39m=\u001b[39mbatch_examples)\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:443\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 443\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39;49m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[1;32m    444\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[1;32m    445\u001b[0m         ]\n\u001b[1;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_batch(batch_examples\u001b[39m=\u001b[39mbatch_examples)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m ds\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m data: datasets\u001b[39m.\u001b[39mDataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_pandas(ds)\n\u001b[0;32m----> 6\u001b[0m ds \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      7\u001b[0m     process_dataset,\n\u001b[1;32m      8\u001b[0m     new_fingerprint\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39masdasdasd\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:563\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 563\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    564\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    565\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    566\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:528\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    522\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    523\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    526\u001b[0m }\n\u001b[1;32m    527\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 528\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    529\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    530\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2953\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2945\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2946\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   2947\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   2948\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2951\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2952\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 2953\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   2954\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   2955\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:3358\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3356\u001b[0m \u001b[39mif\u001b[39;00m update_data:\n\u001b[1;32m   3357\u001b[0m     \u001b[39mif\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 3358\u001b[0m         writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[1;32m   3359\u001b[0m     \u001b[39mif\u001b[39;00m tmp_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3360\u001b[0m         tmp_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:582\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[0;34m(self, close_stream)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 582\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[1;32m    583\u001b[0m \u001b[39m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema:\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:442\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m array_concat(arrays)\n\u001b[1;32m    441\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[1;32m    443\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[1;32m    444\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[1;32m    445\u001b[0m         ]\n\u001b[1;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_batch(batch_examples\u001b[39m=\u001b[39mbatch_examples)\n\u001b[1;32m    447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_writer.py:443\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    440\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m array_concat(arrays)\n\u001b[1;32m    441\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         batch_examples[col] \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 443\u001b[0m             row[\u001b[39m0\u001b[39m][col]\u001b[39m.\u001b[39mto_pylist()[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row[\u001b[39m0\u001b[39;49m][col], (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)) \u001b[39melse\u001b[39;00m row[\u001b[39m0\u001b[39m][col]\n\u001b[1;32m    444\u001b[0m             \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples\n\u001b[1;32m    445\u001b[0m         ]\n\u001b[1;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_batch(batch_examples\u001b[39m=\u001b[39mbatch_examples)\n\u001b[1;32m    447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_ids'"
     ]
    }
   ],
   "source": [
    "ds = pd.read_csv(path, header=None, delimiter='NODELIMITERUSEDHEREJUSTREADLINEBYLINE')\n",
    "ds.columns = ['text']\n",
    "data: datasets.Dataset = datasets.Dataset.from_pandas(ds)\n",
    "\n",
    "\n",
    "ds = data.map(\n",
    "    process_dataset,\n",
    "    new_fingerprint='asdasdasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
