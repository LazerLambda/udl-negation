{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "model: AutoModelForMaskedLM = AutoModelForMaskedLM.from_pretrained(\n",
    "            'roberta-base')\n",
    "optimizer: AdamW = AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'output_attentions': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': None,\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': True,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': ['RobertaForMaskedLM'],\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': 0,\n",
       " 'pad_token_id': 1,\n",
       " 'eos_token_id': 2,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': 'roberta-base',\n",
       " 'transformers_version': '4.26.1',\n",
       " 'model_type': 'roberta',\n",
       " 'vocab_size': 50265,\n",
       " 'hidden_size': 768,\n",
       " 'num_hidden_layers': 12,\n",
       " 'num_attention_heads': 12,\n",
       " 'hidden_act': 'gelu',\n",
       " 'intermediate_size': 3072,\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'max_position_embeddings': 514,\n",
       " 'type_vocab_size': 1,\n",
       " 'initializer_range': 0.02,\n",
       " 'layer_norm_eps': 1e-05,\n",
       " 'position_embedding_type': 'absolute',\n",
       " 'use_cache': True,\n",
       " 'classifier_dropout': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/philko/.cache/huggingface/datasets/text/default-28d9029d0517ca66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3975c33c3c494ccf8c883d095bef4421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "path: str = \"/home/philko/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/data/processed/roberta_filtered/debug.txt\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"text\", data_files={\"train\": path}, sample_by=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070c578ba47b44be8b9692806109d9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8911928351244bdba61f2a7c52d6a334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_test = dataset['train'].train_test_split(test_size=0.01)\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': train_test['test']})\n",
    "tokenize_function = lambda examples: tokenizer(examples[\"text\"])\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd95d94b24d4b81a2c74d214d1b2b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 1. Reducing num_proc to 1 for dataset of size 1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b593aeda87494f59b7b96c708f37497a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0,\n",
       "  417,\n",
       "  18045,\n",
       "  56,\n",
       "  450,\n",
       "  475,\n",
       "  12231,\n",
       "  129,\n",
       "  2330,\n",
       "  11,\n",
       "  39,\n",
       "  7370,\n",
       "  12,\n",
       "  627,\n",
       "  183,\n",
       "  37,\n",
       "  56,\n",
       "  57,\n",
       "  2421,\n",
       "  8,\n",
       "  5,\n",
       "  183,\n",
       "  37,\n",
       "  376,\n",
       "  184,\n",
       "  31,\n",
       "  5,\n",
       "  1098,\n",
       "  479,\n",
       "  71,\n",
       "  14,\n",
       "  2156,\n",
       "  37,\n",
       "  56,\n",
       "  295,\n",
       "  75,\n",
       "  57,\n",
       "  2509,\n",
       "  11,\n",
       "  143,\n",
       "  9,\n",
       "  5,\n",
       "  3493,\n",
       "  8,\n",
       "  5575,\n",
       "  162,\n",
       "  3494,\n",
       "  1051,\n",
       "  479,\n",
       "  19,\n",
       "  39,\n",
       "  2038,\n",
       "  1037,\n",
       "  756,\n",
       "  15,\n",
       "  5,\n",
       "  1430,\n",
       "  2156,\n",
       "  385,\n",
       "  18045,\n",
       "  56,\n",
       "  295,\n",
       "  75,\n",
       "  770,\n",
       "  7,\n",
       "  28,\n",
       "  26623,\n",
       "  1329,\n",
       "  19,\n",
       "  5,\n",
       "  9582,\n",
       "  9,\n",
       "  10,\n",
       "  1928,\n",
       "  479,\n",
       "  1386,\n",
       "  2156,\n",
       "  37,\n",
       "  770,\n",
       "  7,\n",
       "  1930,\n",
       "  39,\n",
       "  86,\n",
       "  160,\n",
       "  5,\n",
       "  882,\n",
       "  27865,\n",
       "  454,\n",
       "  70,\n",
       "  722,\n",
       "  9,\n",
       "  5,\n",
       "  363,\n",
       "  479,\n",
       "  37,\n",
       "  129,\n",
       "  1199,\n",
       "  920,\n",
       "  323,\n",
       "  77,\n",
       "  162,\n",
       "  3494,\n",
       "  3711,\n",
       "  7,\n",
       "  33,\n",
       "  39,\n",
       "  6729,\n",
       "  28788,\n",
       "  6555,\n",
       "  479,\n",
       "  2,\n",
       "  0,\n",
       "  119,\n",
       "  12231,\n",
       "  21,\n",
       "  416,\n",
       "  18446,\n",
       "  160,\n",
       "  5,\n",
       "  12201,\n",
       "  11,\n",
       "  6958,\n",
       "  8,\n",
       "  2408,\n",
       "  309,\n",
       "  7,\n",
       "  39,\n",
       "  18559],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [0,\n",
       "  417,\n",
       "  18045,\n",
       "  56,\n",
       "  450,\n",
       "  475,\n",
       "  12231,\n",
       "  129,\n",
       "  2330,\n",
       "  11,\n",
       "  39,\n",
       "  7370,\n",
       "  12,\n",
       "  627,\n",
       "  183,\n",
       "  37,\n",
       "  56,\n",
       "  57,\n",
       "  2421,\n",
       "  8,\n",
       "  5,\n",
       "  183,\n",
       "  37,\n",
       "  376,\n",
       "  184,\n",
       "  31,\n",
       "  5,\n",
       "  1098,\n",
       "  479,\n",
       "  71,\n",
       "  14,\n",
       "  2156,\n",
       "  37,\n",
       "  56,\n",
       "  295,\n",
       "  75,\n",
       "  57,\n",
       "  2509,\n",
       "  11,\n",
       "  143,\n",
       "  9,\n",
       "  5,\n",
       "  3493,\n",
       "  8,\n",
       "  5575,\n",
       "  162,\n",
       "  3494,\n",
       "  1051,\n",
       "  479,\n",
       "  19,\n",
       "  39,\n",
       "  2038,\n",
       "  1037,\n",
       "  756,\n",
       "  15,\n",
       "  5,\n",
       "  1430,\n",
       "  2156,\n",
       "  385,\n",
       "  18045,\n",
       "  56,\n",
       "  295,\n",
       "  75,\n",
       "  770,\n",
       "  7,\n",
       "  28,\n",
       "  26623,\n",
       "  1329,\n",
       "  19,\n",
       "  5,\n",
       "  9582,\n",
       "  9,\n",
       "  10,\n",
       "  1928,\n",
       "  479,\n",
       "  1386,\n",
       "  2156,\n",
       "  37,\n",
       "  770,\n",
       "  7,\n",
       "  1930,\n",
       "  39,\n",
       "  86,\n",
       "  160,\n",
       "  5,\n",
       "  882,\n",
       "  27865,\n",
       "  454,\n",
       "  70,\n",
       "  722,\n",
       "  9,\n",
       "  5,\n",
       "  363,\n",
       "  479,\n",
       "  37,\n",
       "  129,\n",
       "  1199,\n",
       "  920,\n",
       "  323,\n",
       "  77,\n",
       "  162,\n",
       "  3494,\n",
       "  3711,\n",
       "  7,\n",
       "  33,\n",
       "  39,\n",
       "  6729,\n",
       "  28788,\n",
       "  6555,\n",
       "  479,\n",
       "  2,\n",
       "  0,\n",
       "  119,\n",
       "  12231,\n",
       "  21,\n",
       "  416,\n",
       "  18446,\n",
       "  160,\n",
       "  5,\n",
       "  12201,\n",
       "  11,\n",
       "  6958,\n",
       "  8,\n",
       "  2408,\n",
       "  309,\n",
       "  7,\n",
       "  39,\n",
       "  18559]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older . she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age . `` are n't you being a good boy ? '' she said . mason barely acknowledged her .\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_function = lambda examples: tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/philko/.cache/huggingface/datasets/text/default-28d9029d0517ca66/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-8faa653c20f7c62b_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "113\n",
      "111\n",
      "90\n",
      "101\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for sent in tokenized_datasets['train']['input_ids']:\n",
    "    print(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenized_datasets['train']['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>he 'd seen the movie almost by mistake, considering he was a little young for the pg cartoon, but with older cousins, along with her brothers, mason was often exposed to things that were older. she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age. `` aren't you being a good boy? '' she said. mason barely acknowledged her.</s>\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138a2d45b8ee47dda50716a932d3d7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  700,\n",
       "  128,\n",
       "  417,\n",
       "  450,\n",
       "  5,\n",
       "  1569,\n",
       "  818,\n",
       "  30,\n",
       "  5021,\n",
       "  2156,\n",
       "  2811,\n",
       "  37,\n",
       "  21,\n",
       "  10,\n",
       "  410,\n",
       "  664,\n",
       "  13,\n",
       "  5,\n",
       "  47194,\n",
       "  15800,\n",
       "  2156,\n",
       "  53,\n",
       "  19,\n",
       "  2530,\n",
       "  20005,\n",
       "  2156,\n",
       "  552,\n",
       "  19,\n",
       "  69,\n",
       "  5396,\n",
       "  2156,\n",
       "  475,\n",
       "  12231,\n",
       "  21,\n",
       "  747,\n",
       "  4924,\n",
       "  7,\n",
       "  383,\n",
       "  14,\n",
       "  58,\n",
       "  2530,\n",
       "  479,\n",
       "  79,\n",
       "  6640,\n",
       "  7,\n",
       "  206,\n",
       "  145,\n",
       "  7501,\n",
       "  30,\n",
       "  3362,\n",
       "  8,\n",
       "  2530,\n",
       "  1159,\n",
       "  21,\n",
       "  65,\n",
       "  1219,\n",
       "  596,\n",
       "  37,\n",
       "  21,\n",
       "  10,\n",
       "  215,\n",
       "  10,\n",
       "  205,\n",
       "  1067,\n",
       "  254,\n",
       "  13,\n",
       "  39,\n",
       "  1046,\n",
       "  479,\n",
       "  45518,\n",
       "  32,\n",
       "  295,\n",
       "  75,\n",
       "  47,\n",
       "  145,\n",
       "  10,\n",
       "  205,\n",
       "  2143,\n",
       "  17487,\n",
       "  12801,\n",
       "  79,\n",
       "  26,\n",
       "  479,\n",
       "  475,\n",
       "  12231,\n",
       "  6254,\n",
       "  5055,\n",
       "  69,\n",
       "  479,\n",
       "  2,\n",
       "  0,\n",
       "  119,\n",
       "  12231,\n",
       "  21,\n",
       "  416,\n",
       "  18446,\n",
       "  160,\n",
       "  5,\n",
       "  12201,\n",
       "  11,\n",
       "  6958,\n",
       "  8,\n",
       "  2408,\n",
       "  309,\n",
       "  7,\n",
       "  39,\n",
       "  18559,\n",
       "  811,\n",
       "  479,\n",
       "  385,\n",
       "  18045,\n",
       "  56,\n",
       "  450,\n",
       "  475,\n",
       "  12231,\n",
       "  129,\n",
       "  2330,\n",
       "  11,\n",
       "  39,\n",
       "  7370,\n",
       "  12,\n",
       "  627,\n",
       "  183,\n",
       "  37,\n",
       "  56,\n",
       "  57,\n",
       "  2421],\n",
       " [0,\n",
       "  417,\n",
       "  18045,\n",
       "  56,\n",
       "  450,\n",
       "  475,\n",
       "  12231,\n",
       "  129,\n",
       "  2330,\n",
       "  11,\n",
       "  39,\n",
       "  7370,\n",
       "  12,\n",
       "  627,\n",
       "  183,\n",
       "  37,\n",
       "  56,\n",
       "  57,\n",
       "  2421,\n",
       "  8,\n",
       "  5,\n",
       "  183,\n",
       "  37,\n",
       "  376,\n",
       "  184,\n",
       "  31,\n",
       "  5,\n",
       "  1098,\n",
       "  479,\n",
       "  71,\n",
       "  14,\n",
       "  2156,\n",
       "  37,\n",
       "  56,\n",
       "  295,\n",
       "  75,\n",
       "  57,\n",
       "  2509,\n",
       "  11,\n",
       "  143,\n",
       "  9,\n",
       "  5,\n",
       "  3493,\n",
       "  8,\n",
       "  5575,\n",
       "  162,\n",
       "  3494,\n",
       "  1051,\n",
       "  479,\n",
       "  19,\n",
       "  39,\n",
       "  2038,\n",
       "  1037,\n",
       "  756,\n",
       "  15,\n",
       "  5,\n",
       "  1430,\n",
       "  2156,\n",
       "  385,\n",
       "  18045,\n",
       "  56,\n",
       "  295,\n",
       "  75,\n",
       "  770,\n",
       "  7,\n",
       "  28,\n",
       "  26623,\n",
       "  1329,\n",
       "  19,\n",
       "  5,\n",
       "  9582,\n",
       "  9,\n",
       "  10,\n",
       "  1928,\n",
       "  479,\n",
       "  1386,\n",
       "  2156,\n",
       "  37,\n",
       "  770,\n",
       "  7,\n",
       "  1930,\n",
       "  39,\n",
       "  86,\n",
       "  160,\n",
       "  5,\n",
       "  882,\n",
       "  27865,\n",
       "  454,\n",
       "  70,\n",
       "  722,\n",
       "  9,\n",
       "  5,\n",
       "  363,\n",
       "  479,\n",
       "  37,\n",
       "  129,\n",
       "  1199,\n",
       "  920,\n",
       "  323,\n",
       "  77,\n",
       "  162,\n",
       "  3494,\n",
       "  3711,\n",
       "  7,\n",
       "  33,\n",
       "  39,\n",
       "  6729,\n",
       "  28788,\n",
       "  6555,\n",
       "  479,\n",
       "  2,\n",
       "  0,\n",
       "  700,\n",
       "  129,\n",
       "  1199,\n",
       "  920,\n",
       "  323,\n",
       "  77,\n",
       "  162,\n",
       "  3494,\n",
       "  3711,\n",
       "  7,\n",
       "  33,\n",
       "  39,\n",
       "  6729,\n",
       "  28788,\n",
       "  6555,\n",
       "  479]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['train']['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 3 is out of bounds for size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[39m.\u001b[39mdecode(lm_datasets[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]), tokenizer\u001b[39m.\u001b[39mdecode(lm_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m3\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]),\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2658\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2657\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:2642\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2640\u001b[0m format_kwargs \u001b[39m=\u001b[39m format_kwargs \u001b[39mif\u001b[39;00m format_kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m   2641\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2642\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data, key, indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_indices \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   2643\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2644\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39mformatter, format_columns\u001b[39m=\u001b[39mformat_columns, output_all_columns\u001b[39m=\u001b[39moutput_all_columns\n\u001b[1;32m   2645\u001b[0m )\n\u001b[1;32m   2646\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/formatting/formatting.py:588\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     size \u001b[39m=\u001b[39m indices\u001b[39m.\u001b[39mnum_rows \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m table\u001b[39m.\u001b[39mnum_rows\n\u001b[0;32m--> 588\u001b[0m     _check_valid_index_key(key, size)\n\u001b[1;32m    589\u001b[0m \u001b[39m# Query the main table\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39mif\u001b[39;00m indices \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/datasets/formatting/formatting.py:531\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m    530\u001b[0m     \u001b[39mif\u001b[39;00m (key \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m key \u001b[39m+\u001b[39m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m size):\n\u001b[0;32m--> 531\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is out of bounds for size \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    532\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Invalid key: 3 is out of bounds for size 2"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][0][\"input_ids\"]), tokenizer.decode(lm_datasets[\"train\"][3][\"input_ids\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135fc040afe74d64b7482e283b924cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)\"pytorch_model.bin\";:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-wikitext2\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=2,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philko/Documents/Uni/WiSe2223/UnsupervisedLearning/udl-negation/.venv/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n",
      "  Number of trainable parameters = 124697433\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1a63c5d9914ea4a2049fb3d8064c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9216832b5c423b832679779707659b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.568877220153809, 'eval_runtime': 1.273, 'eval_samples_per_second': 1.571, 'eval_steps_per_second': 0.786, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4482b16e0b8345dd9ef1e90869e45ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.8370513916015625, 'eval_runtime': 1.3339, 'eval_samples_per_second': 1.499, 'eval_steps_per_second': 0.75, 'epoch': 2.0}\n",
      "{'train_runtime': 12.9934, 'train_samples_per_second': 0.308, 'train_steps_per_second': 0.154, 'train_loss': 7.557348251342773, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=7.557348251342773, metrics={'train_runtime': 12.9934, 'train_samples_per_second': 0.308, 'train_steps_per_second': 0.154, 'train_loss': 7.557348251342773, 'epoch': 2.0})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4cd9546fb95363a717c3e804955b7f788fc5499a59fd4f7b2fcbeda3fd0ff3ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
